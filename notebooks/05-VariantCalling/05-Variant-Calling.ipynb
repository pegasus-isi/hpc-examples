{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant Calling Pegasus Workflow\n",
    "\n",
    "In this notebook, we will create Pegasus Workflow corresponding to the  [Automating a Variant Calling Workflow](https://datacarpentry.org/wrangling-genomics/05-automation.html) \n",
    "from Data Carpentry Lesson \n",
    "[Data Wrangling and Processing for Genomics](https://datacarpentry.org/wrangling-genomics/).\n",
    "\n",
    "This workflow downloads and aligns SRA data to the E. coli  REL606 reference genome, and checks what differences exist in our reads versus the genome. The workflow also performs perform  variant calling to see how \n",
    "the population changed over time. \n",
    "\n",
    "One major change from other examples is that this workflow, uses CARC project storage to do the\n",
    "data staging. The previous examples, uses the your directory space on CARC which is limited to 100GB.\n",
    "\n",
    "## Container\n",
    "\n",
    "All tools required to execute the jobs in the container are all included in\n",
    "a single Docker container defined in `docker/Dockerfile` and available in the\n",
    "[Docker Hub](https://hub.docker.com/repository/docker/pegasus/variant-calling) under\n",
    "`pegasus/variant-calling`. The workflow is setup up to use that container\n",
    "but execute it via Singularity as that maybe a more common container\n",
    "runtime on HPC machines. The container runtime used can easily be\n",
    "changed in the workflow definition.\n",
    "\n",
    "The container comes with the following tools\n",
    "* Burrows-Wheeler Aligner (BWA) 0.7.17\n",
    "* SamTools 1.15.1\n",
    "* Bcftools 1.15.1\n",
    "* HTSlib   1.15.1\n",
    "* SRA Tools 3.0.0\n",
    "\n",
    "The number of concurrent downloads is limited with a DAGMan\n",
    "category profile.\n",
    "\n",
    "## CARC Access\n",
    "\n",
    "The Center for Advanced Researcfh Computing provides a service intended to facilitate computational research.\n",
    "\n",
    "If you dont have access to an existing CARC account. Please refer to the instructions on the \n",
    "[Accounts and Allocations Page In CARC Guide](https://www.carc.usc.edu/user-information/accounts)\n",
    "\n",
    "## Workflow\n",
    "\n",
    "The Pegasus workflow downloads SRA data from NCBI repository using\n",
    "`fasterq-dump` in the SRA toolkit and aligns it against the reference genome.\n",
    "\n",
    "Please note, that using more than one thread in `fasterq-dump` will result in your job hanging up due to the filesystem bug.\n",
    "\n",
    "![Pegasus Variant Calling Workflow for 2 SRA reads](../images/variant-calling-pegasus-workflow.png)\n",
    "\n",
    "The tools used for various jobs in the worklfow are listed in table below\n",
    "\n",
    "| Job Label                 | Tool Used        |\n",
    "| --------------------------|----------------- |\n",
    "| bwa                       | bwa              |\n",
    "| fasterq_dump              | fasterq_dump     |\n",
    "| align_reads               | bwa              |\n",
    "| sam_2_bam_converter       | samtools         |\n",
    "| calculate_read_coverage   | bcftools         |\n",
    "| detect_snv                | bcftools         |\n",
    "| variant_calling           | vcfutils         |\n",
    "\n",
    "The bwa invocation as part of the align_reads job is configured to use multiple cores. In this notebook, we set the core usage for that job to 6 and enable use of threads( set to 12 - an oversubscription) on the command line to the bwa job. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set Jupyter Environment\n",
    "\n",
    "We set some environment variables and set PYTHONPATH for Pegasus libraries to be imported successfully.\n",
    "This is temporary until the Jupyter Notebook setup is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/lib64/python3.6/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LANG=en_US.utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the Variant Calling Workflow\n",
    "\n",
    "By now, you have a good idea about the Pegasus Workflow API.\n",
    "We now create the workflow for the Variant Calling based on the picture above\n",
    "\n",
    "First step is to identify what SRA we want to pull down from the NCBI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sra_list = [\"SRR2584866\",\n",
    "            \"SRR2589044\"]\n",
    "reference_genome=\"./ref_genome/ecoli_rel606.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = str(Path(\".\").resolve())\n",
    "\n",
    "# need to know where Pegasus is installed for notifications\n",
    "PEGASUS_HOME = shutil.which('pegasus-version')\n",
    "PEGASUS_HOME = os.path.dirname(os.path.dirname(PEGASUS_HOME))\n",
    "\n",
    "wf = Workflow('variant-calling')\n",
    "tc = TransformationCatalog()\n",
    "rc = ReplicaCatalog()\n",
    "sc = SiteCatalog()\n",
    "    \n",
    "# --- Properties ----------------------------------------------------------\n",
    "\n",
    "# set the concurrency limit for the download jobs, and send some extra usage\n",
    "# data to the Pegasus developers\n",
    "props = Properties()\n",
    "props['dagman.fasterq-dump.maxjobs'] = '10'\n",
    "props['pegasus.catalog.workflow.amqp.url'] = 'amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows'\n",
    "props['pegasus.data.configuration'] = 'nonsharedfs'\n",
    "props[\"pegasus.mode\"] = \"tutorial\"\n",
    "props.write() \n",
    "\n",
    "# --- Event Hooks ---------------------------------------------------------\n",
    "\n",
    "# get emails on all events at the workflow level\n",
    "wf.add_shell_hook(EventType.ALL, '{}/share/pegasus/notification/email'.format(PEGASUS_HOME))\n",
    "\n",
    "# --- Transformations -----------------------------------------------------\n",
    "\n",
    "container = Container(\n",
    "               'variant-calling',\n",
    "               Container.SINGULARITY,\n",
    "               'docker://pegasus/variant-calling:latest',\n",
    "               mounts=[\"{}/work:{}/work:rw\".format(BASE_DIR, BASE_DIR)]\n",
    "            )\n",
    "tc.add_containers(container)\n",
    "\n",
    "fasterq_dump = Transformation(\n",
    "    'fasterq-dump',\n",
    "    site='local',\n",
    "    container=container,\n",
    "    pfn=BASE_DIR + '/tools/fasterq_dump_wrapper',\n",
    "    is_stageable=True\n",
    ")\n",
    "fasterq_dump.add_profiles(Namespace.CONDOR, key='request_memory', value='6 GB')\n",
    "# this one is used to limit the number of concurrent downloads\n",
    "fasterq_dump.add_profiles(Namespace.DAGMAN, key='category', value='fasterq-dump')\n",
    "tc.add_transformations(fasterq_dump)\n",
    "\n",
    "bwa = Transformation(\n",
    "                   'bwa',\n",
    "                   site='incontainer',\n",
    "                   container=container,\n",
    "                   pfn='/opt/software/install/bwa/default/bwa',\n",
    "                   is_stageable=False\n",
    "                )\n",
    "bwa.add_profiles(Namespace.CONDOR, key='request_memory', value='6 GB')\n",
    "tc.add_transformations(bwa)\n",
    "\n",
    "# we use the simple bash wrapper to convert to bam,\n",
    "# sort and index the generated bam file\n",
    "samtools = Transformation(\n",
    "    'samtools',\n",
    "    site='local',\n",
    "    container=container,\n",
    "    pfn=BASE_DIR + '/tools/samtools_wrapper',\n",
    "    is_stageable=True\n",
    ")\n",
    "samtools.add_profiles(Namespace.CONDOR, key='request_memory', value='6 GB')\n",
    "tc.add_transformations(samtools)\n",
    "\n",
    "bcftools = Transformation(\n",
    "    'bcftools',\n",
    "    site='incontainer',\n",
    "    container=container,\n",
    "    pfn='/opt/software/install/bcftools/default/bin/bcftools',\n",
    "    is_stageable=False\n",
    ")\n",
    "bcftools.add_profiles(Namespace.CONDOR, key='request_memory', value='6 GB')\n",
    "tc.add_transformations(bcftools)\n",
    "\n",
    "vcfutils = Transformation(\n",
    "    'vcfutils',\n",
    "    site='incontainer',\n",
    "    container=container,\n",
    "    pfn='/opt/software/install/bcftools/default/bin/vcfutils.pl',\n",
    "    is_stageable=False\n",
    ")\n",
    "vcfutils.add_profiles(Namespace.CONDOR, key='request_memory', value='6 GB')\n",
    "tc.add_transformations(vcfutils)\n",
    "\n",
    "# --- Site Catalog ------------------------------------------------- \n",
    "# add a local site with an optional job env file to use for compute jobs\n",
    "shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "\n",
    "# some variables for slurm cluster. you may wish to update\n",
    "# them for your needs\n",
    "slurm_partition=\"epyc-64\"\n",
    "slurm_account=\"osinski_982\"\n",
    "\n",
    "local = Site(\"local\") \\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\n",
    "\n",
    "slurm_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "slurm_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "\n",
    "slurm = Site(\"slurm\")\\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, slurm_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + slurm_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, slurm_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + slurm_storage_dir, Operation.ALL)))\n",
    "\n",
    "slurm.add_pegasus_profile(\n",
    "                        style=\"glite\",\n",
    "                        queue=slurm_partition,\n",
    "                        project=slurm_account,\n",
    "                        data_configuration=\"nonsharedfs\",\n",
    "                        auxillary_local=\"true\",\n",
    "                        nodes=1,\n",
    "                        ppn=1,\n",
    "                        runtime=6400,\n",
    "                        clusters_num=2\n",
    "                    )\n",
    "slurm.add_condor_profile(grid_resource=\"batch slurm\")\n",
    "\n",
    "sc = SiteCatalog()\n",
    "sc.add_sites(local)\n",
    "sc.add_sites(slurm)\n",
    "sc.write() # written to ./sites.yml\n",
    "# --- Workflow -----------------------------------------------------\n",
    "# set up the reference genome and what files need to be generated by the index job\n",
    "ref_genome_lfn=os.path.basename(reference_genome)\n",
    "ref_genome = File(ref_genome_lfn)\n",
    "rc.add_replica('local', ref_genome_lfn, os.path.abspath(reference_genome))\n",
    "index_files = []\n",
    "for suffix in ['amb', 'ann', 'bwt', 'pac', 'sa']:\n",
    "    index_files.append(File(ref_genome.lfn + \".\" + suffix))\n",
    "\n",
    "# index the reference file\n",
    "index_job = Job('bwa', node_label=\"ref_genome_index\")\n",
    "index_job.add_args('index', ref_genome.lfn)\n",
    "index_job.add_inputs(ref_genome)\n",
    "index_job.add_outputs(*index_files, stage_out=False)\n",
    "wf.add_jobs(index_job)\n",
    "\n",
    "# create jobs for each trimmed fastq trim.sub.fastq\n",
    "for sra in sra_list:\n",
    "    sra_id = sra.strip()\n",
    "    if len(sra_id) < 5:\n",
    "        continue\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # files for this id\n",
    "    # commented out as we download files from NCBI as part of fasterq-dump job\n",
    "    fastq_1 = File('{}_1.trim.sub.fastq'.format(sra_id))\n",
    "    fastq_2 = File('{}_2.trim.sub.fastq'.format(sra_id))\n",
    "    rc.add_replica('local', fastq_1, os.path.join(os.path.abspath(args.fastq_dir), fastq_1.lfn))\n",
    "    rc.add_replica('local', fastq_2, os.path.join(os.path.abspath(args.fastq_dir), fastq_2.lfn))\n",
    "    \"\"\"\n",
    "\n",
    "    sam=File('{}.aligned.sam'.format(sra_id))\n",
    "    bam=File('{}.aligned.bam'.format(sra_id))\n",
    "    sorted_bam=File('{}.aligned.sorted.bam'.format(sra_id))\n",
    "\n",
    "    raw_bcf=File('{}_raw.bcf'.format(sra_id))\n",
    "    variants=File('{}_variants.bcf'.format(sra_id))\n",
    "    final_variants=File('{}_final_variants.bcf'.format(sra_id))\n",
    "\n",
    "    \"\"\"\n",
    "    bwa mem $genome $fq1 $fq2 > $sam\n",
    "    samtools view -S -b $sam > $bam\n",
    "    samtools sort -o $sorted_bam $bam \n",
    "    samtools index $sorted_bam\n",
    "    bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam\n",
    "    bcftools call --ploidy 1 -m -v -o $variants $raw_bcf \n",
    "    vcfutils.pl varFilter $variants > $final_variants\n",
    "    \"\"\"\n",
    "\n",
    "    # files for this id\n",
    "    fastq_1 = File('{}_1.fastq'.format(sra_id))\n",
    "    fastq_2 = File('{}_2.fastq'.format(sra_id))\n",
    "\n",
    "    # download job\n",
    "    j = Job('fasterq-dump', node_label=\"fasterq_dump\")\n",
    "    j.add_pegasus_profile(cores=6)\n",
    "    j.add_args('--split-files', sra_id)\n",
    "    j.add_args('-e', '6')\n",
    "    j.add_outputs(fastq_1, fastq_2, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # align reads tp reference genome job\n",
    "    j = Job('bwa', node_label=\"align_reads\")\n",
    "    j.add_pegasus_profile(cores=10)\n",
    "    j.add_args('mem', \"-t 10\", ref_genome, fastq_1, fastq_2)\n",
    "    j.add_inputs(*index_files, ref_genome, fastq_1, fastq_2)\n",
    "    j.set_stdout(sam, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # samtools_wrapper for doing alignment to genome\n",
    "    j = Job('samtools', node_label=\"sam_2_bam_converter\")\n",
    "    j.add_args(sra_id)\n",
    "    j.add_inputs(sam)\n",
    "    j.add_outputs(bam, sorted_bam, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # Variant calling\n",
    "    # bcftools for calculating the read coverage of positions in the genome\n",
    "    j = Job('bcftools', node_label=\"calculate_read_coverage\")\n",
    "    j.add_pegasus_profile(cores=10)\n",
    "    j.add_args('mpileup --threads 10 -O b -o', raw_bcf, '-f', ref_genome, sorted_bam)\n",
    "    j.add_inputs(ref_genome, sorted_bam)\n",
    "    j.add_outputs(raw_bcf, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # bcftools for Detect the single nucleotide variants (SNVs)\n",
    "    j = Job('bcftools', node_label=\"detect_snv\")\n",
    "    j.add_pegasus_profile(cores=10)\n",
    "    j.add_args('call --threads 10 --ploidy 1 -m -v -o', variants, raw_bcf)\n",
    "    j.add_inputs(raw_bcf)\n",
    "    j.add_outputs(variants, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # vcfutils Filter and report the SNV variants in variant calling format (VCF)\n",
    "    j = Job('vcfutils', node_label=\"variant_calling\")\n",
    "    j.add_args('varFilter', variants)\n",
    "    j.add_inputs(variants)\n",
    "    j.set_stdout(final_variants, stage_out=True)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "wf.add_transformation_catalog(tc)\n",
    "wf.add_site_catalog(sc)\n",
    "wf.add_replica_catalog(rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Workflow\n",
    "\n",
    "Once you have defined your abstract workflow, you can use `pegasus-graphviz` to visualize it. `Workflow.graph()` will invoke `pegasus-graphviz` internally and render your workflow using one of the available formats such as `png`. **Note that Workflow.write() must be invoked before calling Workflow.graph().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plan and Submit the Workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. You will notice below that we have added a new option staging_sites to plan the workflow. This option tells Pegasus to use **slurm** site for data staging when running jobs on site **slurm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(sites=[\"slurm\"], verbose=1,submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line in the output that starts with pegasus-status, contains the command you can use to monitor the status of the workflow. We will cover this command line tool in the next couple of notbooks. The path it contains is the path to the submit directory where all of the files required to submit and monitor the workflow are stored. For now we will just continue to use the Python `Workflow` object.\n",
    "\n",
    "## 5. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  What's next\n",
    "\n",
    "Next Notebook is `06-Summary`, that summarizes what we have learnt so far and how to request further support."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
